---
title: Representation learning 
---

Oct 09
: [Pretraining and finetuning (basics)](https://nyu-cs2590.github.io/course-material/fall2024/lecture/lec06/main.pdf) [[recording]]
  : **HW 2 due**{: .label .label-red }
    **HW 3 out**{: .label .label-green} [HW3 zip](https://github.com/nyu-cs2590/course-material/blob/gh-pages/fall2024/assignment/hw3/hw3.zip?raw=true) [[pdf]](https://nyu-cs2590.github.io/course-material/fall2024/assignment/hw3/hw3.pdf)

: - Self-supervised learning
  - Encoder-only, decoder-only, encoder-decoder models
  - Efficient pre-training 

Oct 10           
: **Section**{: .label .label-purple } Huggingface Transformer [sec06 notebook](https://github.com/nyu-cs2590/course-material/blob/gh-pages/fall2024/section/sec06/sec06.ipynb)

Oct 16
: [Pretraining and finetuning (advanced)]() [[recording]]
  : **Proposal due**{: .label .label-red }
: - Sub-word tokenization 
  - Optimization algorithms
  - Mixed-precision training
  - Parameter efficient finetuning 

Oct 17           
: **Section**{: .label .label-purple } Position encoding 
