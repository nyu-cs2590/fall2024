---
title: Supervised learning 
---

Sep 4
: [Text classification]() [[recording]]
  : **HW 1 out**{: .label .label-green }
: - Course overview
  - Supervised learning basics
  - Feature-based text classification
  - Additional readings: [JM Ch4.1](https://web.stanford.edu/~jurafsky/slp3/4.pdf), [JM Ch5](https://web.stanford.edu/~jurafsky/slp3/5.pdf)

Sep 5 
: **Section**{: .label .label-purple } Text classification 

Sep 11 
: [Word embedding]() [[recording]]
  : 
: - Distributed representation of words
  - Learning word vectors
  - Additional readings: [JM Ch6](https://web.stanford.edu/~jurafsky/slp3/6.pdf), [Efficient estimation of word representations in vector space](https://arxiv.org/pdf/1301.3781) (original word2vec paper)

Sep 12
: **Section**{: .label .label-purple } Word vector algebra 

Sep 18 
: [Sequence modeling]() [[recording]]
  : **HW 1 due**{: .label .label-red}
  : **HW 2 out**{: .label .label-green}
: - Neural network basics
  - RNN and its variants 
  - Attention and Transformers
  - Additional readings: [D2L Ch9.4-9.7](https://d2l.ai/chapter_recurrent-neural-networks/index.html), [D2L 10.1](https://d2l.ai/chapter_recurrent-modern/lstm.html), [D2L Ch11.1-11.7](https://d2l.ai/chapter_recurrent-neural-networks/index.html), [Attention is all you need](https://arxiv.org/pdf/1706.03762) (original Transformer paper)

Sep 19           
: **Section**{: .label .label-purple } HPC and PyTorch tutorial 

Sep 25
: [Sequence generation]() [[recording]]
    : 
: - Encoder-decoder models
  - Decoding algorithms

Sep 26
: **Section**{: .label .label-purple } Machine translation and encoder-decoder models

Oct 2
: [Tasks and applications]() [[recording]]
  : 
: - The landscape of NLP tasks
  - Final project tips 

Oct 3           
: **Section**{: .label .label-purple } Data processing, Huggingface datasets, Datasheet 
